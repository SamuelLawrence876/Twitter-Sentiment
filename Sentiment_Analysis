import re
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import GetOldTweets3 as got
import seaborn as sns
import datetime
import os
import nltk
import fasttext
from textblob import TextBlob
import cv2 


# ## Data Extraction

today = datetime.date.today()
yesterday = today - datetime.timedelta(days=1)
week = today - datetime.timedelta(days=7)
Three_Days = datetime.timedelta(days=3)
text_query = '$TSLA'
since_date = str(week)
until_date = str(today)
count = 10000
Location = 'London, United Kingdom'
Distance = '200mi'


tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query)    .setSince(since_date).setUntil(until_date).setMaxTweets(count).setLang('en')



# Creation of list that contains all tweets
tweets = got.manager.TweetManager.getTweets(tweetCriteria)



# Creating list of chosen tweet data
text_tweets = [[tweet.date, tweet.text, tweet.permalink, tweet.id] for tweet in tweets]


df = pd.DataFrame(data=text_tweets, columns = ['Time', 'Tweet','Link','ID'])



pd.set_option('display.max_colwidth', None)
df.shape



df.sample(5)


df.shape

df['Time'].iloc[0]

df['Tweet'].iloc[0]


# ## Data Prep / Cleaning

df['cleanLinks'] = df['Tweet'].apply(lambda x: re.split('https:\/\/.*', str(x))[0]) #Removing URL
df['cleanLinks'] = df['cleanLinks'].apply(lambda x: x.lower()) #Lower text


spec_chars = ["!",'"',"#","%","&","'","(",")",
              "*","+",",","-",".","/",":",";","<",
              "=",">","?","@","[","\\","]","^","_",
              "`","{","|","}","~","â€“",'$']


for char in spec_chars:
    df['cleanLinks'] = df['cleanLinks'].str.replace(char, ' ')



df['cleanLinks'][1]


# Remove word stoppers? 


# ## Applying stock market sentiment

df['Sentiment'] = df['cleanLinks'].apply(lambda x: TextBlob(x))


df['Sentiment'] = df['Sentiment'].apply(lambda x: x.sentiment)


df['Polarity'] = df['Sentiment'].apply(lambda x: re.split(',', str(x))[0])
df['Polarity'] = df['Polarity'].apply(lambda x: re.split('=', str(x))[1])
df['Polarity'] = df['Polarity'].apply(lambda x: float(x))


df['Subjectivity'] = df['Sentiment'].apply(lambda x: re.split(',', str(x))[1])
df['Subjectivity'] = df['Subjectivity'].apply(lambda x: re.split('=', str(x))[1])
df['Subjectivity'] = df['Subjectivity'].apply(lambda x: x.strip(')'))
df['Subjectivity'] = df['Subjectivity'].apply(lambda x: float(x))


# ## Sentiment without:
# Changeing all tweets to lowercase
# Removeing all stopwords (as defined by nltk)
# Removeing mentions, links, and hashtags
# Removeing all punctuation and brackets
# Removeing all one-character words

df.sample(5)


# ### Polarity / Subjectivity before removing subjective tweets 

df['Polarity'].mean()


df['Subjectivity'].mean()

df.shape

#Droping High Subjectivity 
df = df[df.Subjectivity > 0.5] 


# ### Polarity / Subjectivity after removing subjective tweets 

df['Polarity'].mean()


df['Subjectivity'].mean()

df.shape


# ### Short Long Position count

def Wordcount(cleanLinks):
    if 'buy' in cleanLinks.lower():
        return 'buy positions'
    if 'sell' in cleanLinks.lower():
        return 'sell positions'    
    if 'short' in cleanLinks.lower():
        return 'short positions'    
    if 'long' in cleanLinks.lower():
        return 'long positions'
    if 'put' in cleanLinks.lower():
        return 'put positions'
    if 'call' in cleanLinks.lower():
        return 'call positions'
    else: 
        return


df['Market Position'] = df['cleanLinks'].apply(Wordcount)



df['Market Position'].value_counts()

df['Market Position'].hist(figsize=(10,5))


#### Machine Learning Testing


from sklearn.model_selection import train_test_split


x_train, x_test = train = train_test_split(df['cleanLinks'], test_size = .33, random_state = 42) 

from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


#img = cv2.imread('10wmt-superJumbo-v4.jpg')
#mask = img



words = " ".join(df['cleanLinks'])
def punctuation_stop(text):
    """remove punctuation and stop words"""
    filtered = []
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    for w in word_tokens:
        if w not in stop_words and w.isalpha():
            filtered.append(w.lower())
    return filtered

words_filtered = punctuation_stop(words)
unwanted = ['stock', 'market','moving','average','economy','stockmarket','stocks','tsla','']
text = " ".join([ele for ele in words_filtered if ele not in unwanted])
wc= WordCloud(background_color="gray", random_state=1,stopwords=STOPWORDS, max_words = 500, width =2000, height = 2000)
wc.generate(text)
plt.figure(figsize=[10,10])
plt.imshow(wc, interpolation="bilinear")
plt.axis('off')
plt.show()



df.to_csv('TwitterData.csv')

